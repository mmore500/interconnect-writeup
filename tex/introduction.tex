\section{Introduction}

The challenge, and promise, of open-ended evolution has animated decades of inquiry and discussion within the artificial life community \citep{packard2019overview}.
The difficulty of devising models that produce characteristic outcomes of open-ended evolution indicates we must have profound philosophical or scientific blind spots in our understanding of the natural processes that gave rise to contemporary organisms and ecosystems.
Already, pursuit of open-ended evolution has yielded paradigm-shifting insights.
For example, novelty search, which demonstrated how processes promoting non-adaptive diversification can ultimately yield adaptive outcomes that were previously unattainable \citep{lehman2011abandoning}.
Such work lends insight to fundamental questions in evolutionary biology, such as the role --- or agnosticism -- of natural selection with respect to increases in complexity \citep{lehman2012evolution, lynch2007frailty} and the origins of evolvability \citep{lehman2013evolvability, kirschner1998evolvability}.
Evolutionary algorithms devised in support of open-ended evolution models also promise to deliver tangible broader impacts on society. %and evolutionary artifacts generated by these models
Possibilities include the generative design of engineered constructs, consumer products, art, video games, and AI systems \citep{nguyen2015innovation, stanley2017open}.

Preceding decades have witnessed advances towards defining --- quantitatively and philosophically --- the concept of open-ended evolution \citep{lehman2012beyond, dolson2019modes, bedau1998classification}, as well as investigating causal phenomena that promote it such as ecological dynamics, selection, and evolvability \citep{dolson2019constructive, soros2014identifying, huizinga2018emergence}.
Together, methodological and theoretical advances have begun to yield evidence that the generative potential of artificial life systems is --- at least in part --- meaningfully constrained by available compute resources \citep{channon2019maximum}. % talk to Alex about this`

Since approximately the turn of the century, advances in the clock speed of traditional serial processing have trailed off.
Since that time, have existing technologies begun to encounter fundamental constraints including power use and thermal dissipation \citep{sutter2005free}.
% add sentence on parallel hardware keeping up with moore's law
Scaling up artificial life systems designed to study open-ended evolution will require taking advantage of modern parallel and distributed compute technologies.
% Scaling up artificial life 

In fact, digital evolution practitioners have a rich history of leveraging parallel and distributed hardware.
It is common practice to distribute multiple self-isolated instantiations of evolutionary runs over multiple hardware units.
In scientific contexts, this practice yields replicate datasets that provide statistical power to answer a research question \citep{dolson2017spatial}.
In applied contexts, the 
or,  to generate  the best solution from them (TODO cite).

Another established practice is to use ``island models'' where individuals are transplanted between populations that are otherwise independently evolving across distributed hardware.
Koza and collaborators' genetic programming work with a 1,000-cpu Beowulf cluster typifies this approach \citep{bennett1999building}.

In recent years, Sentient Technologies spearheaded digital evolution projects on an unprecedented computational scale, comprising over a million CPUs and capable of a peak performance of 9 petaflops \citep{miikkulainen2019evolving}.
According to its proponents, the scale and scalability of this DarkCycle system was a key aspect of its conceptualization \citep{gilbert_2015}.
Much of the assembled infrastructure was pieced together from heterogeneous providers and employed on a time-available basis \citep{blondeau2012distributed}.
Unlike the island model where selection events are performed independently on each CPU, this scheme apparently involved the dynamic transfer of evaluation criteria between computational instances (in addition to individual genomes) \citep{hodjat2013distributed}.

Also notable from Sentient Technologies was large-scale use of many massively-parallel hardware units (e.g., 100 GPUs) to evaluate the performance of candidate deep learning neural network architectures on image classification, language modeling, and image captioning problems.
Hardware parallelism accelerated the deep learning training process used to evaluate individual solutions \citep{miikkulainen2019evolving}.
Analogous work parallelizing the evaluation of an evolutionary individual over multiple test cases in the context of genetic programming has used GPU hardware and vectorized CPU operations \citep{harding2007fast2, langdon2019continuous}. %TODO check this sentence

%TODO add subsections throughout introduction (~one column)
% \citep{langdon2010large} breast cancer

%TODO the system should be capable of distributing individuals?
% individuals evolving in environments where they have to deal with the issues of being distributed (but not necessarily distributed on hardware although they could be)
% * implementation of system for efficiency
% * agents creating algorithms to deal with being distributedy
% "leaves the door open"
With respect to incorporating parallel and distributed hardware in the context of open-ended evolution, we should likewise aim to distribute and concurrently evaluate computational elements constituting an evolutionary individual.
However --- unlike most existing applications of parallel and distributed computing to digital evolution --- we should also prioritize dynamic interactions between and within individuals.
% rephrase tongue twister
% add citations
% generative developmental processes (very broad, think about)
% reading from one genome in multiple instances allows for repetitions, symmetries, hierarchy (cite Clune?) 
Dynamic interaction between these concurrent components constituting an individual allows for generative developmental processes, understood as key to evolvability, and the production of emergent functionality among components.
Importantly, our approach to incorporating parallel and distributed hardware should also provide for dynamic interactions between contemporary evolving individuals.
Such interactions are understood as key to ecologies, co-evolution, and social interaction. %TODO cite

% TODO check towards
David Ackley has led toward such a distributed approach, laying out an ambitious vision for modular hardware at a theoretically unlimited scale \citep{ackley2011pursue} and demonstrating an algorithmic substrate for emergent agents that can take advantage of it \citep{ackley2018digital}.
% text from blog post!

% come with high level step through (reverse outline)

%(TODO I've heard similar discussion made before somewhere, cite it)
While by no means a certainty, the idea is that orders-of-magnitude differences in compute power will open up qualitatively different possibilities with respect to open-ended evolution is also not entirely unfounded.
Analogy to spectacular advances achieved with artificial neural networks over the last decade illuminates a possible path towards this outcome.
As with digital evolution, artificial neural networks (ANNs) were traditionally understood as a highly-versatile but auxiliary methodology --- both techniques been described along the lines of "the second best way to do almost anything" \citep{miaoulis2008intelligent, eiben2015introduction}.
However, the utility and ubiquity of ANNs has since increased dramatically.
The development AlexNet is widely considered pivotal to this transformation.
AlexNet united methodological innovations from the field (such as big datasets, dropout, and ReLU) with GPU computing that enabled training of orders-of-magnitude-larger networks.
In fact, some aspects of aspects of their deep learning architecture were expressly modified to accommodate multi-GPU training \citep{krizhevsky2012imagenet}.
By adapting existing methodology to accommodate --- and take advantage of --- commercially available hardware AlexNet spurred the greater availability of compute resources to the research domain and eventually the introduction of custom hardware to expressly support deep learning \citep{jouppi2017datacenter}.

AlexNet set into motion within the field a cycle of repeatedly earning continued investment and developing methodology, software, and eventually hardware to take advantage of additional resources as well as continuing advances of on-silicon design and manufacturing.
In addition to developing hardware-agnostic theory and methodology, we believe that pushing the envelope of open-ended evolution will analogously require expressly designing systems to effectively leverage existing commercially-available parallel and distributed compute resources at circumstantially-feasible scales.
Modern high-performance scientific computing clusters appear perhaps the best target to start down this path.
These systems combine
\begin{itemize}
\item memory-sharing parallel architectures comprising dozens of cores (commonly targeted using OpenMP \citep{dagum1998openmp})
\item and low-latency high-throughput message-passing between distributed nodes (commonly targeted using MPI\citep{clarke1994mpi}).
\end{itemize}

In absolute terms such clusters lack in key characteristics highlighted for indefinite scalability: fault tolerance, arbitrary extensibility, and steady asynchronous operation.
However, at current scale --- and into the forseeable future --- they also offer opportunities not available in a indefinitely scalable framework: log-time interconnects.
TODO cite this, is this true?

Many natural systems --- such as ecosystems, genetic regulatory networks, and neural networks --- are known to exhibit small-world patterns of connectivity between components \citep{bassett2017small, fox2014herbivores, gaiteri2014beyond}.
In small-world graphs, mean path length (the number of edges traversed on a shortest-route) between arbitrary components scales logarithmically with system size \citep{watts1998collective}.
We anticipate that open-ended phenomena emerging across distributed hardware might also involve small-world connectivity dynamics.
As such a system scales, what implications might such connectivity have with respect to load on individual hardware components?
With respect to the latency and bandwidth of inter-component interactions as such a system scales?
What would the impact be of providing a system of hierarchical log-time hardware interconnects as opposed to relying solely on local hardware interconnects?

In Section \ref{sec:results}, we analyze the scaling relationship between system size and expected node-to-node hops traversed between computational elements interacting as part of an emergent small-world network
\footnote{
Although relativistic concerns do ultimately limit latency between spatially-distributed computational elements, with respect to current hardware co-located at a single physical site at foreseeable scales we see minimizing node-to-node hops as key.
Additionally, although we focus on asymptotic analyses, in some cases better scaling coefficients might be achieved with long-distance hardware interconnects.
Maximizing performance in this regard is also an important goal.
}
\begin{enumerate}
\item with and without hierarchical log-time physical interconnects between computational nodes, and
\item with computational nodes embedded on one-, two-, or three-dimensional computational meshes.
\end{enumerate}

In Section \ref{sec:proof1}, we find that expected hops over edges weighted by edge betweenness centrality scales polynomially in all cases without hierarchical physical interconnects.
With hierarchical physical interconnects, a logarithmic scaling relationship can be achieved.

In Section \ref{sec:proof2} and \ref{sec:proof3} we find that hierarchical physical interconnects yield better best-case mean hops per edge in the one-dimensional case.
Interestingly, asymptotically better outcomes in two- and three- dimensions cannot be guaranteed by hierarchical physical interconnects.
This suggests that --- even at truly vast scales --- emergent inter-component interaction networks could arise with bounded per-hardware-component messaging load.

In Section \ref{sec:proof4} we show that, with a specific traditional construction of small-world graphs, best-case mean hops per edge scales polynomially with graph size.
With hierarchical physical interconnects, a logarithmic scaling relationship can be achieved.

What could a system that under the hood relies on computationally-efficient long-distance hardware interconnects?
One option is TODO that facilitates system-managed interconnects () look like?
In Section \ref{sec:casestudy}, we turn to a case study to look at how an artificial life system might establish small-world interactions between computational elements distributed on different hardware nodes and take advantage of hierarchical physical interconnects.

We present an extension to upcoming work incorporating genetic programming with the DISHTINY platform for transitions in individuality \citep{moreno2019toward}.
This extension provides cells that are laid out on a grid with the capability --- through exploratory growth --- to establish explicit, direct interconnects with spatially distant cells.
We report a case study of where adaptive resource-sharing and messaging emerged over these interconnects.
Our preliminary implementation uses shared-memory thread-level parallelism.

DISHTINY is ... TODO
Although designed for scalability largely along the lines outlined by Ackley, our approach exchanges a uniform, evolutionary-passive substrate for manually-engineered self-replicators.
Evolutionary transitions in individuality provide a framework to unite self-replicators and induce meaningful functional synthesis of programmatic components tied to individual compute elements.
The system is designed to expose the underlying hardware reality (e.g., procedural expression of programs, hierarchical interconnects) so that the hardware capabilities can be fully taken advantage of.
As well as engineering-over complex features (interconnects, genetic transmission of information) along the lines of Channon

\begin{displayquote}
It is not computationally feasible (even if we knew how) for an OEE simulation to start from a sparse fog of hydrogen and helium and transition to a biological-level era, so it is clearly necessary to skip over or engineer in at least some complex features that arose through major transitions in our universe. \citep{channon2019maximum}
\end{displayquote}

become ascendant when different orders of magnitude of more compute power --- to the extent available by Ackley --- become available, and current work sets the stage for that eventuality. \citep{ackley2018alife}
This will address important questions in its own right about the computational (?) foundations of physical and biological reality.
